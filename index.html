<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation">
  <meta name="keywords" content="Behaviour Cloning, Imitation Learning, Robot Learning, IMLE, Generative Modelling">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://kit.fontawesome.com/7a7a763338.js" crossorigin="anonymous"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">IMLE Policy:</br>Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation</h1>
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://krishanrana.github.io">Krishan Rana*</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com.au/citations?user=1Vqlm0kAAAAJ&hl=en">Robert Lee*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/david-pershouse-b74b88285/?originalSubdomain=au">David Pershouse*</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://nikosuenderhauf.github.io/">Niko Suenderhauf</a><sup>1</sup>,
            </span>
          </div> -->

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>QUT Centre for Robotics</span>
            <span class="author-block"><sup>2</sup>Sydekick Robotics</span>
          </div> -->

          <!-- <div class="column has-text-centered">
            <p>*Equal Contribution</p>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-brands fa-x-twitter"></i>
                  </span>
                  <span>Thread</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



  
<!-- <section class="section" style="padding-top: 8px; margin-top: 0;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <p>&#128204 <strong>Conference on Robot Learning (CoRL), 2023</strong> <span style="color:red; font-weight:bold;">(Oral Talk)</span></p>
        <p>&#128204 <strong>Robotics: Science and Systems (RSS) Robot Representations For Scene Understanding, Reasoning and Planning, 2023</strong> <span style="color:red; font-weight:bold;">(Oral Talk)</span></p>
      </div>
    </div>
  </div>
</section> -->
  

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline height="100%">
        <source src="Images/rs_imle_main_video_x2.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">SayPlan</span> scales the grounding of task plans generated by large language models to <b>multi-room</b> and <b>multi-floor</b> environments using 3D scene graph representations.
      </h2>
    </div>
  </div>
</section> -->



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="Images/main.png" alt="semantic search gif">
    </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">IMLE Policy</span> can learn complex, multimodal action distributons from a <b>small number of demonstrations</b> while only requiring a <b>single inference step</b> for action generation.
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in imitation learning, particularly using generative modelling techniques like diffusion, have enabled policies to capture complex multi-modal action distributions. However, these methods often require large datasets and multiple inference steps for action generation, posing challenges in robotics where the cost for data collection is high and computation resources are limited. To address this, we introduce IMLE Policy, a novel behaviour cloning approach based on Implicit Maximum Likelihood Estimation (IMLE). IMLE Policy excels in low-data regimes, effectively learning from minimal demonstrations and requiring 38% less data on average to match the performance of baseline methods in learning complex multi-modal behaviours. Its simple generator-based architecture enables single-step action generation, improving inference speed by 97.3% compared to Diffusion Policy, while outperforming single-step Flow Matching. We validate our approach across diverse manipulation tasks in simulated and real-world environments, showcasing its ability to capture complex behaviours under data constraints.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Approach</h2>

        <div class="content has-text-justified">
            <p style="text-align:center;">
                <image src="Images/imle_policy_main.png" class="img-responsive">        	   
            </p>
          <p>
            <p><strong>IMLE Policy Overview</strong> 
              a) Training: The policy takes in a sequence of past observations 
              <span>&#x1D4AA;</span> and <em>m</em> sampled latents <strong>z</strong> for which the policy generates 
              <em>m</em> sequences of predicted actions <span>&#x1D4A0;</span>. Generated trajectories that lie 
              within the rejection sampling threshold <strong>&epsilon;</strong> are rejected. From the remaining 
              trajectories, the nearest-neighbour to the ground truth trajectory is selected for training. 
              We minimise the distance between this trajectory and the ground truth trajectory to optimise 
              the policy. As the loss focuses on each data sample, it ensures that all modes are captured 
              even from small datasets. 
              b) When compared to baselines with similar multi-modal capturing capabilities, IMLE can 
              generate actions with a single inference step as opposed to multi-step de-noising processes. 
              c) For highly multi-modal tasks, we enhance the performance of IMLE Policy by introducing 
              a simple inference procedure to induce consistency upon mode choice based on a nearest-neighbour 
              search over batch-generated action proposals with the previously executed action sequence.
           </p>
           
          </p>
        </div>



        <section class="hero teaser">
          <div class="container is-max-desktop">
            <div class="hero-body" style="display: flex; flex-direction: column; align-items: center; text-align: center;">
              <video id="teaser" autoplay controls muted loop playsinline style="max-width: 100%; height: auto;">
                <source src="Images/rs_imle_main_video_x2.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                <br/>
                <span class="dnerf">IMLE Policy</span> consistently outperforms all baselines in low data regimes, capturing complex multi-modal action distributions with a single inference step.
              </h2>
            </div>
          </div>
        </section>
        

        
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
          <!-- <h3 class="title is-4">Semantic Search</h3>
          <div class="columns">
              <div class="column is-8">
                  <img src="Images/search.gif" alt="semantic search gif">
              </div>
              <div class="column is-4" style="display: flex; align-items: center;">
                  <div class="content has-text-justified">
                      <p>
                          Given a natural language instruction, the LLM conducts a semantic search for a task-relevant subgraph which contains the assets and objects that are required for planning. This is done by manipulating the nodes of a <i>collapsed</i> 3D scene graph through <code>expand</code> and <code>contract</code> API function calls -- thus making it feasible to plan over increasingly large-scale environments. In doing so, the LLM maintains focus on the small, informative subgraph, during planning, without exceeding its token limit.
                      </p>
                  </div>
              </div>
          </div> -->
    <!--/ Animation. -->
    <!-- <h3 class="title is-4" style="text-align: left;">Iterative Re-planning</h3>
          <div class="columns">
            <div class="column is-4" style="display: flex; align-items: center;">
              <div class="content has-text-justified">
                <p>
                  To ensure the executability of the proposed plan, we iteratively verify the plan using feedback from a scene graph simulator to correct for any unexecutable actions such as -  missing to open the fridge before placing something into it; thus avoiding planning failures due to inconsistencies, hallucinations, or violations of the physical constraints imposed by the environment. We additionally relax the need for the LLM to produce long navigation sequences by incorporating an existing path planner such as Dijkstra to connect high-level nodes generated by the LLM.
                </p>
              </div>
            </div>
            <div class="column is-8">
              <img src="Images/planning.gif" alt="semantic search gif">
            </div>
          </div>
      </div>
    </div> -->

    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <div class="content has-text-justified">
          <p>
            We demonstrate the ability to autonomously deploy the generated plan on a real mobile manipulator robot using a visually grounded motion controller. We utilise a pre-constructed 3D scene graph of an office floor spanning 36 rooms and containing 150 different assets and objects that the agent can interact with. Note the ability of the agent to generate grounded plans across multiple rooms. In all the videos shown, the robot operates fully autonomously using visual feedback from its in-hand camera for manipulation and localises itself within the scene graph using its onboard laser scanner.
          </p>

        </div>
      </div>
    </div> -->

    <!-- <section class="hero is-light is-small">
        <div class="hero-body">
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item item-steve">
                <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
                  <source src="Images/peter_banana_final.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-chair-tp">
                <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
                  <source src="Images/chip_niko_replace.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-chair-tp">
                <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
                  <source src="Images/three_bottles_final.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-shiba">
                <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
                  <source src="Images/soda_spill_simple.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-fullbody">
                <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
                  <source src="Images/dimity_stapler.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>
      </section> -->
    

  </div>
</section>


<!-- <section class="section" id="BibTeX">
<div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{
        rana2023sayplan,
        title={SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning},
        author={Krishan Rana and Jesse Haviland and Sourav Garg and Jad Abou-Chakra and Ian Reid and Niko Suenderhauf},
        booktitle={7th Annual Conference on Robot Learning},
        year={2023},
        url={https://openreview.net/forum?id=wMpOMO0Ss7a}
      }</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>

    <div class="columns is-centered">
      <div class="column is-8">
<!--         <div class="institute-logos has-text-centered"> -->
        <div class="content has-text-centered">
          <!-- <img src="./Images/qcr.png" alt="Institute 1" style="width:200px; margin:20px;"> -->
          <!-- <img src="./Images/uoa.png" alt="Institute 2" style="width:200px; margin:20px;"> -->
          <!-- <img src="./Images/data61.png" alt="Institute 3" style="width:200px; margin:20px;"> -->
        </div>
      </div>
    </div>


      <div class="columns is-centered">
      <div class="column is-8">
       <div class="content has-text-centered"> 
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
          </p>
      </div> 
      </div>
    </div>
    
  </div>
</footer>

</body>
</html>
